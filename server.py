stir
stirbabywindstrumentals
Online

stir ‚Äî 10/29/2023 10:09 PM
Image
Image
Image
Image
Image
Image
Image
Image
Image
stir ‚Äî 11/06/2023 2:45 PM
jubdyn-heXwo2-zedmym
stir ‚Äî 11/10/2023 10:26 PM
https://tinyurl.com/bdxe3nvv
Google Docs
StirlingCarterResume2023.docx
Stirling Carter github.com/stirlingcarter +1 336-501-7556 stirlingcarter@gmail.com EDUCATION Vanderbilt University 2015 - 2019Nashville, TN ‚Ä¢ BS, Computer Science; BA, Mathematics; Board of Dir...
stir ‚Äî 11/10/2023 11:09 PM
https://www.metacareers.com/jobs/1021414515577599/
Meta Careers
Software Engineer, Product
Meta's mission is to give people the power to build community and bring the world closer together. Together, we can help people build stronger communities - join us.
https://www.metacareers.com/jobs/334196349265342/
Meta Careers
Software Engineer, Product
Meta's mission is to give people the power to build community and bring the world closer together. Together, we can help people build stronger communities - join us.
stir ‚Äî 11/12/2023 10:17 PM
M: Hi Sammie you seem very wonderful just getting wonderful vibes

S: i'm very glad to hear ‚úåÔ∏è

M: We could listen to plugg
or even have a nice cup of tea together

S: we could do both

M: I want to go to a nice tea house together
I will give you an airpod
1 have very clean ears

S: nice
when are you free ü§î

M: I'm free tomorrow evenin
I'm free tomorrow evenin and also
Wednesday
Possibly later this week as well and you??
hmm
we'll problem is i'm leaving for home for thanksgiving soon
won't be back until the 24th
it's an opportunity for a beautiful talking stage
Would you care for my number so you can text or ft while you're gone
u right
yee, send over
Yay
it's 336 501 7556
hii
sammie here
Hi!
Where's your family at btw
are you excited to see them or is it gonna be a long trip haha
i made it a shorter trip on purpose
love them, just visited home so often this year lol
fams in the bay area!
You go a lot huh do you miss the bay
Im bay-curious once a season I'm like imma move up there then I don't lol
I should probs decide if l'm gonna visit but same I've been a lot and they're all the way on the east coast smh
Delivered
stir ‚Äî 11/30/2023 12:13 PM
http://soundcloud.com/ilya-grindstaff
SoundCloud
Ilya Grindstaff
Ilya Grindstaff
stir ‚Äî 01/06/2024 6:49 PM
jubdyn-heXwo2-zedmym
stir ‚Äî 01/16/2024 8:27 PM
Image
Image
Image
Image
Image
Image
Image
Image
Image
Image
stir ‚Äî 02/21/2024 7:55 PM
https://meet.google.com/apt-vtoy-hua
Meet
Real-time meetings by Google. Using your browser, share your video, desktop, and presentations with teammates and customers.
Image
stir ‚Äî 02/22/2024 1:39 AM
Year 19:
monthly cost: 5183.535364050574
rental profit: 3404.8661224798093
this is like a rental burden of 1778.6692415707644
monthly etf contributions: 2521.3307584292365
ETFs: 1349940.0591991113
Expand
message.txt
8 KB
stir ‚Äî 02/24/2024 4:27 PM
https://docs.google.com/document/d/1Zuqwo3HtwKzml1d_AiXXB_3_vSLL5qt9r6a30VOaM2o/edit
Google Docs
Agreement
stir ‚Äî 02/25/2024 12:56 PM
https://docs.google.com/spreadsheets/d/1Dyzo1klB0y0uSSsnXDbQ_ogQ_W3h3NQtatgNyyGMEeo/edit?usp=sharing
Google Docs
Amortization schedule
Sheet1

80k principal at 5% interest
Month,Interest,Principal,Ending Balance
1,$333.33,$1,176.37,$78,823.63
2,$328.43,$1,181.27,$77,642.37
3,$323.51,$1,186.19,$76,456.18
4,$318.57,$1,191.13,$75,265.05
5,$313.60,$1,196.09,$74,068.95
6,$308.62,$1,201.08,$72,867.88
7,$303.62,$1,206.08,$71,661.79
8,$...
stir ‚Äî 02/27/2024 6:34 PM
"DATES","Assets"
"January 2020","$9,576.70"
"February 2020","$7,239.30"
"March 2020","$9,000.11"
"April 2020","$9,788.38"
"May 2020","$7,842.95"
Expand
trends.csv
2 KB
stir ‚Äî 03/06/2024 5:40 PM
https://www.themls.com/Share/YWFkaWJkaGZj
stir ‚Äî 04/07/2024 9:47 PM
https://nodeflair.com/blog/bytedance-software-engineer-interview-questions-and-process
Bytedance Software Engineer Interview Questions and Process - Compi...
We made a Cheat Sheet after examining 243 interviews for Bytedance Software Engineering roles!
Bytedance Software Engineer Interview Questions and Process - Compi...
stir ‚Äî 04/12/2024 10:11 PM
RH 1099
Attachment file type: acrobat
Robinhood_Markets_Consolidated_Form_1099.pdf
36.06 KB
SHAWB 1099
stir ‚Äî 05/15/2024 12:14 AM
https://ww4.fmovies.co/film/blackberry-1630855258/
Fmovies
Watch BlackBerry Full Movie on FMovies.to
Watch BlackBerry Online Full Movie without registration. Super fast streaming in 1080p of BlackBerry on Fmovies
Image
stir ‚Äî 05/16/2024 11:25 AM
Attachment file type: acrobat
StirlingCarterResume05162024.pdf
86.75 KB
stir ‚Äî 05/16/2024 10:46 PM
from collections import defaultdict, deque

class TreeNode:
    def init(self, val=0, left=None, right=None):
        self.val = val
        self.left = left
        self.right = right

class Solution:
    def verticalOrder(self, root: TreeNode):
        if not root:
            return []

        column_table = defaultdict(list)
        queue = deque([(root, 0)])

        while queue:
            node, column = queue.popleft()
            if node is not None:
                column_table[column].append(node.val)
                queue.append((node.left, column - 1))
                queue.append((node.right, column + 1))

        return [column_table[x] for x in sorted(column_table.keys())]
stir ‚Äî 06/07/2024 2:59 PM
Attachment file type: acrobat
StirlingCarterResume2024.docx-2.pdf
86.05 KB
stir ‚Äî 06/18/2024 7:08 PM
Situation:
After a couple years at SC, I noticed a significant issue affecting our user experience. SoundCloud operates with two disjoint domains: one for listeners and one for creators. The creator domain, inherited from an acquisition, had separate models and infrastructure. This separation caused redundant and confusing workflows for users.

Task:
My task was to integrate these domains to create a seamless experience for artists. Specifically, I wanted any track uploaded to SoundCloud by an artist to immediately be visible on the creator domain for distribution, monetization, and earnings and streaming insights. This required overcoming several challenges:

    ‚Ä¢    SoundCloud‚Äôs microservices were designed for batch jobs, not for serving live data.
    ‚Ä¢    The data models between the two environments were redundant and contradictory.
    ‚Ä¢    Our earnings and streaming insights infrastructure had scaling issues and couldn‚Äôt handle additional load.

Action:

    ‚Ä¢    My solution centered around federating the models and data providing these experiences. By combining data from both domains into one federated graph, teams across the organization had a unified source of truth for our data. This enabled better decision-making, clarity, and collaboration on how to represent, retain, or merge data.
    ‚Ä¢    To optimize the core microservices hydrating the graph, I leveraged GraphQL‚Äôs optimized traversals. Through instrumentation, I identified bottleneck services and collaborated to scale them horizontally. We also rolled data away from inefficient services and implemented a caching layer on the graph to reduce the number of calls.
    ‚Ä¢    To address the earnings and streaming data challenges, I created a flattened table in BigQuery with flexible row keys for quick lookups, filtering, and sorting, using a Go backend. I also compressed our largest database to 30% of its original size by combining rows where possible. Our old infrastructure relied on costly, precomputed DynamoDB tables, which were expensive, slow, and inflexible. This new approach allowed for lightning-fast data lookups in any desired format at a lower cost.

The outcome was that tracks now show up on our creator product as soon as they‚Äôre uploaded to SoundCloud, without any delay since we‚Äôre using the same microservices.
 Earnings data is not only available immediately but also loads in milliseconds when it used to take over 10 seconds. 
Tracks are sortable, filterable, and text-searchable. Heavy caching ensures that any overheads are only experienced once by users. Overall, many new accessibility improvements, such as infinite scroll, autofilled metadata, and prepopulated WAVs and album artwork, were able to be added. 
This supports our long-term product vision of having a single domain/front end for our product.
stir ‚Äî 06/25/2024 7:17 PM
Image
Image
Image
stir ‚Äî 06/26/2024 3:41 PM
Image
Image
Image
stir ‚Äî 07/22/2024 12:39 PM
J!Csa49k_;tw+pC
J!Csa49k_;tw+pCO
stir ‚Äî 07/23/2024 11:06 AM
Stirling.carter laptop login
stir ‚Äî 08/11/2024 2:33 PM
Image
stir ‚Äî 08/11/2024 3:46 PM
Image
Image
stir ‚Äî 08/11/2024 3:55 PM
32 ƒê∆∞·ªùng T√¥ Ng·ªçc V√¢n, T√¢y H·ªì, H√† N·ªôi, Vietnam
Sucessfully completed!

Registration code: E240812USAA3487390034
Email: stirlingcarter@gmail.com
Date of birth (DD/MM/YYYY): 04/02/1997
Passport number: A34873900
Nationality: United States of America
Applying date: 12/08/2024

Notes: your registration code must be remembered for looking up e-visa.
stir ‚Äî 09/04/2024 12:01 AM
A4:F6:E8:CF:60:16
stir ‚Äî 09/22/2024 2:40 PM
ws://54.176.45.36:8765
stir ‚Äî Today at 10:17 PM
import os
import sys
import json
import asyncio
import websockets
import logging
import multiprocessing
from vosk import Model, KaldiRecognizer
from openai import OpenAI

Read the API key from key.txt
with open('key.txt', 'r') as file:
    apikey = file.read().strip()

client = OpenAI(apikey=apikey)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(name)

Load Vosk Model
def loadmodel(base_path):
    try:
        model_folder = next(folder for folder in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, folder)))
        return Model(os.path.join(base_path, model_folder))
    except StopIteration:
        logger.error(f"No model found in {base_path}")
        sys.exit(1)

Get recognized completion from OpenAI
async def get_recognized_completion(text):
    prompt = (
        "This is an STT result of a word or sentence(s).\n"
        "1. If you see something obvious (e.g., 'miss steak' instead of 'mistake'), please correct it.\n"
        "2. Show corrections and present both English and Spanish versions.\n"
        "3. Append an arbitrary accuracy score.\n\n"
        f"STT result: {text}"
    )

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}]
    )

    return response.choices[0].message.content

Shared Queue to communicate between processes
async def receive_messages(websocket, rec, shared_queue):
    async for message in websocket:
        if isinstance(message, bytes):
            if rec.AcceptWaveform(message):
                result = json.loads(rec.Result())['text']
                if result:
                    logger.info(f"Recognized: {result}")
                    shared_queue.put(f"Recognized: {result}")

                    # Run the completion in a separate thread
                    completion = await get_recognized_completion(result)
                    logger.info(f"Completion: {completion}")
                    shared_queue.put(f"Completion: {completion}")
            else:
                partial_result = json.loads(rec.PartialResult())['partial']
                if partial_result:
                    logger.info(f"Partial: {partial_result}")
                    shared_queue.put(f"Partial: {partial_result}")
        else:
            logger.warning(f"Received non-bytes message: {message}")

async def publish_messages(websocket, shared_queue):
    while True:
        if not shared_queue.empty():
            message = shared_queue.get()
            await websocket.send(message)
            logger.info(f"Published message: {message}")

def receive_process(shared_queue, model):
    async def handler(websocket, path):
        rec = KaldiRecognizer(model, 16000)
        await receive_messages(websocket, rec, shared_queue)

    start_server = websockets.serve(handler, "0.0.0.0", 8765)
    asyncio.get_event_loop().run_until_complete(start_server)
    asyncio.get_event_loop().run_forever()

def publish_process(shared_queue):
    async def handler(websocket, path):
        await publish_messages(websocket, shared_queue)

    start_server = websockets.serve(handler, "0.0.0.0", 8766)
    asyncio.get_event_loop().run_until_complete(start_server)
    asyncio.get_event_loop().run_forever()

if __name == "__main":
    shared_queue = multiprocessing.Queue()
    model = load_model("../lang/models/")

    # Create and start processes
    receive_p = multiprocessing.Process(target=receive_process, args=(shared_queue, model))
    publish_p = multiprocessing.Process(target=publish_process, args=(shared_queue,))

    receive_p.start()
    publish_p.start()

    receive_p.join()
    publish_p.join()
import os
import sys
import json
import asyncio
import websockets
import logging
import multiprocessing
from vosk import Model, KaldiRecognizer
from openai import OpenAI

# Read the API key from key.txt
with open('key.txt', 'r') as file:
    api_key = file.read().strip()

client = OpenAI(api_key=api_key)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load Vosk Model
def load_model(base_path):
    try:
        model_folder = next(folder for folder in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, folder)))
        return Model(os.path.join(base_path, model_folder))
    except StopIteration:
        logger.error(f"No model found in {base_path}")
        sys.exit(1)

# Get recognized completion from OpenAI
async def get_recognized_completion(text):
    prompt = (
        "This is an STT result of a word or sentence(s).\n"
        "1. If you see something obvious (e.g., 'miss steak' instead of 'mistake'), please correct it.\n"
        "2. Show corrections and present both English and Spanish versions.\n"
        "3. Append an arbitrary accuracy score.\n\n"
        f"STT result: {text}"
    )

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}]
    )

    return response.choices[0].message.content

# Shared Queue to communicate between processes
async def receive_messages(websocket, rec, shared_queue):
    async for message in websocket:
        if isinstance(message, bytes):
            if rec.AcceptWaveform(message):
                result = json.loads(rec.Result())['text']
                if result:
                    logger.info(f"Recognized: {result}")
                    shared_queue.put(f"Recognized: {result}")

                    # Run the completion in a separate thread
                    completion = await get_recognized_completion(result)
                    logger.info(f"Completion: {completion}")
                    shared_queue.put(f"Completion: {completion}")
            else:
                partial_result = json.loads(rec.PartialResult())['partial']
                if partial_result:
                    logger.info(f"Partial: {partial_result}")
                    shared_queue.put(f"Partial: {partial_result}")
        else:
            logger.warning(f"Received non-bytes message: {message}")

async def publish_messages(websocket, shared_queue):
    while True:
        if not shared_queue.empty():
            message = shared_queue.get()
            await websocket.send(message)
            logger.info(f"Published message: {message}")

def receive_process(shared_queue, model):
    async def handler(websocket, path):
        rec = KaldiRecognizer(model, 16000)
        await receive_messages(websocket, rec, shared_queue)

    start_server = websockets.serve(handler, "0.0.0.0", 8765)
    asyncio.get_event_loop().run_until_complete(start_server)
    asyncio.get_event_loop().run_forever()

def publish_process(shared_queue):
    async def handler(websocket, path):
        await publish_messages(websocket, shared_queue)

    start_server = websockets.serve(handler, "0.0.0.0", 8766)
    asyncio.get_event_loop().run_until_complete(start_server)
    asyncio.get_event_loop().run_forever()

if __name__ == "__main__":
    shared_queue = multiprocessing.Queue()
    model = load_model("../lang/models/")

    # Create and start processes
    receive_p = multiprocessing.Process(target=receive_process, args=(shared_queue, model))
    publish_p = multiprocessing.Process(target=publish_process, args=(shared_queue,))

    receive_p.start()
    publish_p.start()

    receive_p.join()
    publish_p.join()
stir ‚Äî Today at 11:37 PM
import os
import sys
import json
import asyncio
import websockets
import logging
import multiprocessing
from vosk import Model, KaldiRecognizer
from openai import OpenAI
import uuid

# Read the API key from key.txt
with open('key.txt', 'r') as file:
    api_key = file.read().strip()

client = OpenAI(api_key=api_key)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load Vosk Model
def load_model(base_path):
    try:
        model_folder = next(folder for folder in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, folder)))
        return Model(os.path.join(base_path, model_folder))
    except StopIteration:
        logger.error(f"No model found in {base_path}")
        sys.exit(1)

# Get recognized completion from OpenAI
async def get_recognized_completion(text):
    prompt = (
        "This is an STT result of a word or sentence(s).\n"
        "1. If you see something obvious (e.g., 'miss steak' instead of 'mistake'), please correct it.\n"
        "2. Show corrections inline with bold or strikethough, or other inline means.\n"
        "3. That's it, feel free not to touch it if it looks good.\n\n"
        f"STT result: {text}"
    )

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}]
    )

    return response.choices[0].message.content

# Shared Queue to communicate between processes
async def receive_messages(websocket, rec, shared_queue, shared_uuid):
    async for message in websocket:
        if isinstance(message, bytes):
            if rec.AcceptWaveform(message):
                result = json.loads(rec.Result())['text']
                if result:
                    logger.info(f"Recognized: {result}")
                    
                    # Create JSON message for recognized text
                    recognized_msg = {
                        "msg": f"{result}",
                        "uuid": shared_uuid.value
                    }

                    shared_queue.put(json.dumps(recognized_msg))

                    completion_msg = {
                        "uuid": shared_uuid.value  # Same UUID as recognized
                    }
                    new_uuid = str(uuid.uuid4())
                    shared_uuid.value = new_uuid  # Update the shared UUID

                    # Run the completion in a separate coroutine
                    completion = await get_recognized_completion(result)
                    
                    logger.info(f"Completion: {completion}")
                    completion_msg["msg"] = f"{completion}"
                    shared_queue.put(json.dumps(completion_msg))
            else:
                partial_result = json.loads(rec.PartialResult())['partial']
                if partial_result:
                    logger.info(f"Partial: {partial_result}")
                    
                    # Use the current UUID for partial messages
                    partial_msg = {
                        "msg": f"{partial_result}",
                        "uuid": shared_uuid.value
                    }
                    shared_queue.put(json.dumps(partial_msg))
        else:
            logger.warning(f"Received non-bytes message: {message}")

async def publish_messages(websocket, shared_queue):
    while True:
        if not shared_queue.empty():
            message = shared_queue.get()
            await websocket.send(message)
            logger.info(f"Published message: {message}")
        else:
            await asyncio.sleep(0.1)  # Prevent busy waiting

def receive_process(shared_queue, shared_uuid, model):
    async def handler(websocket, path):
... (32 lines left)
Collapse
message.txt
5 KB
Ôªø
import os
import sys
import json
import asyncio
import websockets
import logging
import multiprocessing
from vosk import Model, KaldiRecognizer
from openai import OpenAI
import uuid

# Read the API key from key.txt
with open('key.txt', 'r') as file:
    api_key = file.read().strip()

client = OpenAI(api_key=api_key)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load Vosk Model
def load_model(base_path):
    try:
        model_folder = next(folder for folder in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, folder)))
        return Model(os.path.join(base_path, model_folder))
    except StopIteration:
        logger.error(f"No model found in {base_path}")
        sys.exit(1)

# Get recognized completion from OpenAI
async def get_recognized_completion(text):
    prompt = (
        "This is an STT result of a word or sentence(s).\n"
        "1. If you see something obvious (e.g., 'miss steak' instead of 'mistake'), please correct it.\n"
        "2. Show corrections inline with bold or strikethough, or other inline means.\n"
        "3. That's it, feel free not to touch it if it looks good.\n\n"
        f"STT result: {text}"
    )

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}]
    )

    return response.choices[0].message.content

# Shared Queue to communicate between processes
async def receive_messages(websocket, rec, shared_queue, shared_uuid):
    async for message in websocket:
        if isinstance(message, bytes):
            if rec.AcceptWaveform(message):
                result = json.loads(rec.Result())['text']
                if result:
                    logger.info(f"Recognized: {result}")
                    
                    # Create JSON message for recognized text
                    recognized_msg = {
                        "msg": f"{result}",
                        "uuid": shared_uuid.value
                    }

                    shared_queue.put(json.dumps(recognized_msg))

                    completion_msg = {
                        "uuid": shared_uuid.value  # Same UUID as recognized
                    }
                    new_uuid = str(uuid.uuid4())
                    shared_uuid.value = new_uuid  # Update the shared UUID

                    # Run the completion in a separate coroutine
                    completion = await get_recognized_completion(result)
                    
                    logger.info(f"Completion: {completion}")
                    completion_msg["msg"] = f"{completion}"
                    shared_queue.put(json.dumps(completion_msg))
            else:
                partial_result = json.loads(rec.PartialResult())['partial']
                if partial_result:
                    logger.info(f"Partial: {partial_result}")
                    
                    # Use the current UUID for partial messages
                    partial_msg = {
                        "msg": f"{partial_result}",
                        "uuid": shared_uuid.value
                    }
                    shared_queue.put(json.dumps(partial_msg))
        else:
            logger.warning(f"Received non-bytes message: {message}")

async def publish_messages(websocket, shared_queue):
    while True:
        if not shared_queue.empty():
            message = shared_queue.get()
            await websocket.send(message)
            logger.info(f"Published message: {message}")
        else:
            await asyncio.sleep(0.1)  # Prevent busy waiting

def receive_process(shared_queue, shared_uuid, model):
    async def handler(websocket, path):
        rec = KaldiRecognizer(model, 16000)
        await receive_messages(websocket, rec, shared_queue, shared_uuid)

    start_server = websockets.serve(handler, "0.0.0.0", 8765)
    asyncio.get_event_loop().run_until_complete(start_server)
    asyncio.get_event_loop().run_forever()

def publish_process(shared_queue):
    async def handler(websocket, path):
        await publish_messages(websocket, shared_queue)

    start_server = websockets.serve(handler, "0.0.0.0", 8766)
    asyncio.get_event_loop().run_until_complete(start_server)
    asyncio.get_event_loop().run_forever()

if __name__ == "__main__":
    manager = multiprocessing.Manager()
    shared_queue = manager.Queue()
    shared_uuid = manager.Value('c', str(uuid.uuid4()))  # Initialize with a random UUID

    model = load_model("../lang/models/")

    # Create and start processes
    receive_p = multiprocessing.Process(target=receive_process, args=(shared_queue, shared_uuid, model))
    publish_p = multiprocessing.Process(target=publish_process, args=(shared_queue,))

    receive_p.start()
    publish_p.start()

    receive_p.join()
    publish_p.join()
message.txt
5 KB
